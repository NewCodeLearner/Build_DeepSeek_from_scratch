{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOQUtqzWGiZrNF9Ng3XNR+A",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/NewCodeLearner/Build_DeepSeek_from_scratch/blob/main/01_multi_head_latent_attention.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this notebook, we code the Multi-Head Latent Attention (MLA) Mechanism from scratch in Python.\n",
        "\n",
        "MLA was one of the key innovations in the DeepSeek architecture.\n",
        "\n",
        "Here, we code the simplest variant of MLA without rotary positional encodings added."
      ],
      "metadata": {
        "id": "84wyLy2VFTje"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 1: Writing the code for the Multi-Head Latent Attention"
      ],
      "metadata": {
        "id": "KEhDbiHzFaG9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class RopelessMLA(nn.Module):\n",
        "  def __init__(self,d_model,n_heads,kv_latent_dim):\n",
        "    super().__init__()\n",
        "    self.d_model = d_model\n",
        "    self.n_heads = n_heads\n",
        "    self.dh = d_model // n_heads # dimension per head\n",
        "\n",
        "    #Projection layers\n",
        "    self.W_q = nn.Linear(d_model,d_model, bias = False)            # Query Projection\n",
        "    self.W_dkv = nn.Linear(d_model,kv_latent_dim, bias = False)    # Compress into latent KV space\n",
        "    self.W_uk = nn.Linear(kv_latent_dim, d_model, bias = False)    # Decompress K\n",
        "    self.W_uv = nn.Linear(kv_latent_dim,d_model, bias = False)     # Decompress V\n",
        "    self.W_o = nn.Linear(d_model,d_model, bias = False)            # Final output projection\n",
        "\n",
        "    self.ln = nn.LayerNorm(kv_latent_dim)\n",
        "    self.register_buffer('absorbed_k', None)  #Holds W_q @ W_uk\n",
        ""
      ],
      "metadata": {
        "id": "S3DtFeqKFh3h"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}